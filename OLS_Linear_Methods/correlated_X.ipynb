{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ortho_group\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "np.set_printoptions(suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_calc(x,y):\n",
    "    xtx_1 = np.linalg.inv(x.T@x)\n",
    "    beta1 = np.linalg.inv(x.T@x)@(x.T@y)\n",
    "    y_hat = beta1.T@x.T\n",
    "    RSS = np.sum((y-y_hat)**2)\n",
    "    RSE = np.sum(np.sqrt(RSS))\n",
    "    TSS = np.sum((y-np.mean(y))**2)\n",
    "    R2 = ((TSS-RSS)/TSS)\n",
    "    SE = np.sqrt(RSS/(x.shape[0] - x.shape[1]) * xtx_1)\n",
    "\n",
    "    return xtx_1, beta1, y_hat, RSS, RSE, R2, SE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues in OLS to consider and how they manifest\n",
    "\n",
    "I initially wrote this to discuss the [assumptions of OLS](https://gregorygundersen.com/blog/2021/08/26/ols-estimator-sampling-distribution/#standard-ols-assumptions) (linearity, homoscedasticity, no OVB, no multicollinearity, normality of Y), however I took a long-winded turn into convincing myself for the Nth time that collinearity harms our precision and to understand why.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collinearity \n",
    "\n",
    "Below I create a regression of $Y = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\epsilon$ where my $x_{1}$ and $x_{2}$ are highly correlated. \n",
    "\n",
    "Due to the correlation in my covariates we deal with a few issues, practically speaking it will be difficult to understand if $\\beta_{1}$ or $\\beta_{2}$ is responsible for a unit change in $Y$, meaning we're not confident in our estimated values, this is reflected in larger standard error values. \n",
    "\n",
    "On the other hand, our estimates are unbiased! We are still explaining the true relationship between $Y$ and our $X's$, we're just unsure who is responsible for the effect.\n",
    "\n",
    "\n",
    "### What does this mean for us?\n",
    "- If you want to predict, we're fine.\n",
    "\n",
    "- If you want to infer, we're in trouble. \n",
    "\n",
    "- We have true estimates, so we can predict well, but we can't truly determine if our estimators provide a causal impact or not.\n",
    "\n",
    "### What can we do about collinearity? \n",
    "We don't have many options. We can use a number of tests to check if (multi)collinearity exists between our covariates. Typically a __VIF__ test works well, values greater than 10 to 15 might be a cause for concern. \n",
    "\n",
    "A few options in mind:\n",
    "1. Drop if collinearity is very bad.\n",
    "2. If collinearity is not very bad, if prediction is the name of the game, or if the collinear variable carries explanatory power for y then do not drop. \n",
    "3. Combine collinear variables into new variable.\n",
    "\n",
    "\n",
    "Lets look at an example of two collinear variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanity check, x1 & x2 should have high correlation: 0.92\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "rows = 100\n",
    "x = ortho_group.rvs(dim=(rows))\n",
    "x = np.append(np.full((rows,1), 1), x, axis=1)[:,:3]\n",
    "x_6 = x.copy()\n",
    "x_6[:,2] = x_6[:,1] + np.random.rand(rows)*0.15\n",
    "y_6 = np.dot(x_6, [1,1,1]) + np.random.standard_normal(rows)*0.05\n",
    "print('sanity check, x1 & x2 should have high correlation: {:.2f}'.format(np.corrcoef(x_6[:,1:3], rowvar=False)[0,1]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we run our regression with both covariates. Keep the standard error and beta coefficient of $x_{1}$ in mind.\n",
    "\n",
    "*VIF is low because: https://stats.stackexchange.com/questions/445189/why-high-correlation-coefficient-doesnt-guarantee-high-vif#:~:text=As%20demonstrated%2C%20it's%20definitely%20possible,correlations%2C%20but%20have%20high%20VIF's.\n",
    "\n",
    "My take: Have two covariates be mildly/strongly correlated might not be enough to pass the VIF \"sniff test\" (VIF >10 or 15). This is because an R^2 of a single variate regression will need to be extremely correlated to increase the VIF to 10 (think of an R^2 of 0.9). Having a multivariate regression for x1 on all other covariates, even with mild correlation will probably do the trick however.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my coef: [0.99264727 0.84977216 1.09630869]\n",
      "my se: [0.00940184 0.11547962 0.10912604]\n",
      "my r2: 0.9519165234217969\n",
      "VIF Scores:  [4.372849128949262, 6.5932322352189585, 6.593232235218954]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.952</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   960.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 17 Mar 2023</td> <th>  Prob (F-statistic):</th> <td>1.19e-64</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:49:19</td>     <th>  Log-Likelihood:    </th> <td>  169.83</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>  -333.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    97</td>      <th>  BIC:               </th> <td>  -325.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.9926</td> <td>    0.009</td> <td>  105.580</td> <td> 0.000</td> <td>    0.974</td> <td>    1.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.8498</td> <td>    0.115</td> <td>    7.359</td> <td> 0.000</td> <td>    0.621</td> <td>    1.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    1.0963</td> <td>    0.109</td> <td>   10.046</td> <td> 0.000</td> <td>    0.880</td> <td>    1.313</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.480</td> <th>  Durbin-Watson:     </th> <td>   1.806</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.787</td> <th>  Jarque-Bera (JB):  </th> <td>   0.634</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.101</td> <th>  Prob(JB):          </th> <td>   0.728</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.667</td> <th>  Cond. No.          </th> <td>    34.8</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.952\n",
       "Model:                            OLS   Adj. R-squared:                  0.951\n",
       "Method:                 Least Squares   F-statistic:                     960.2\n",
       "Date:                Fri, 17 Mar 2023   Prob (F-statistic):           1.19e-64\n",
       "Time:                        14:49:19   Log-Likelihood:                 169.83\n",
       "No. Observations:                 100   AIC:                            -333.7\n",
       "Df Residuals:                      97   BIC:                            -325.8\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.9926      0.009    105.580      0.000       0.974       1.011\n",
       "x1             0.8498      0.115      7.359      0.000       0.621       1.079\n",
       "x2             1.0963      0.109     10.046      0.000       0.880       1.313\n",
       "==============================================================================\n",
       "Omnibus:                        0.480   Durbin-Watson:                   1.806\n",
       "Prob(Omnibus):                  0.787   Jarque-Bera (JB):                0.634\n",
       "Skew:                           0.101   Prob(JB):                        0.728\n",
       "Kurtosis:                       2.667   Cond. No.                         34.8\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p6_xtx_1, p6_beta1, p6_y_hat, p6_RSS, p6_RSE, p6_R2, p6_SE = hand_calc(x_6,y_6)\n",
    "print('my coef:', p6_beta1)\n",
    "print('my se:', np.diag(p6_SE))\n",
    "print('my r2:', p6_R2)\n",
    "print('VIF Scores: ',[variance_inflation_factor(x_6, i) for i in range(3)])\n",
    "model = sm.OLS(y_6,x_6).fit()\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's drop $x_{2}$ (the collinear variable), now we see that $\\hat{\\beta_{1}}$ has a larger coefficient size and smaller standard error!\n",
    "\n",
    "This is because we're now dealing with omitted variable bias instead of collinearity. This is because I've constructed my $x_1$ and $x_2$ to provide explanatory power for our $y$, even though the two covariates have a strong correlation.\n",
    "\n",
    "We've essentially traded precision for bias, a losing deal for both a causal and prediction task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my coef: [1.07559348 1.9183152 ]\n",
      "my se: [0.0063914 0.063914 ]\n",
      "my r2: 0.9018862348885971\n",
      "VIF Scores:  [4.372849128949262, 6.5932322352189585, 6.593232235218954]\n",
      "\n",
      "After omitting x2, my beta_1 went from 0.8498 to 1.9183, a difference of -1.0685.\n",
      "\n",
      "The standard error of my beta_1 also went from 0.1155 to 0.0639, a difference of 0.0516.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r_x_6 = x_6[:,0:2]\n",
    "r_p6_xtx_1, r_p6_beta1, r_p6_y_hat, r_p6_RSS, r_p6_RSE, r_p6_R2, r_p6_SE = hand_calc(r_x_6,y_6)\n",
    "model = sm.OLS(y_6,r_x_6).fit()\n",
    "print('my coef:', r_p6_beta1)\n",
    "print('my se:', np.diag(r_p6_SE))\n",
    "print('my r2:', r_p6_R2)\n",
    "print('VIF Scores: ',[variance_inflation_factor(x_6, i) for i in range(3)])\n",
    "\n",
    "\n",
    "print('''\n",
    "After omitting x2, my beta_1 went from {:.4f} to {:.4f}, a difference of {:.4f}.\n",
    "\n",
    "The standard error of my beta_1 also went from {:.4f} to {:.4f}, a difference of {:.4f}.\n",
    "'''.format(p6_beta1[1], r_p6_beta1[1], p6_beta1[1]-r_p6_beta1[1],\n",
    "    np.diag(p6_SE)[1], np.diag(r_p6_SE)[1],  np.diag(p6_SE)[1] -  np.diag(r_p6_SE)[1]))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Why the coefficient changed__: Dropping a collinear variable $x_2$ will increase the coefficient of $x_1$ if $x_1$ was correlated with $x_2$ in the original model. This is because, in the original model, some of the effect of $x_1$ on $Y$ was being captured by $x_2$ as well. When $x_2$ is removed from the model, the effect of $x_1$ on $Y$ is no longer shared with $x_2$ and the coefficient of $x_1$ will increase to compensate for this.\n",
    "\n",
    "Put another way, $Y$ is a linear combination of our betas, so $\\beta{0}, \\beta{1}, \\&  \\beta{2}$ would add together and all play a part to explain $Y$. Now that we dropped $\\beta{2}$, we rely on $\\beta{1}$ to pick up the explanatory power. Because these two covariates were correlated its very easy to do so! Here is our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(X'X) with x2:\n",
      "[[100.      0.24    7.8  ]\n",
      " [  0.24    1.      0.993]\n",
      " [  7.8     0.993   1.728]]\n",
      "\n",
      "Without x2:\n",
      "[[100.     0.24]\n",
      " [  0.24   1.  ]]\n",
      "\n",
      "(X'X)^-1 with x2:\n",
      "[[ 0.04372849  0.43202476 -0.44571667]\n",
      " [ 0.43202476  6.59703872 -5.74188313]\n",
      " [-0.44571667 -5.74188313  5.8910836 ]]\n",
      "\n",
      "Without x2:\n",
      "[[ 0.01000577 -0.00240347]\n",
      " [-0.00240347  1.00057733]]\n",
      "\n",
      " Final (x'x)^(-1) * x'y:\n",
      "WITH x2: [0.99264727 0.84977216 1.09630869]\n",
      "WITHOUT x2: [1.07559348 1.9183152 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"(X'X) with x2:\\n{}\\n\\nWithout x2:\\n{}\\n\".format(\n",
    "    (x_6.T@x_6).round(3),\n",
    "    (r_x_6.T@r_x_6).round(3))\n",
    ")\n",
    "\n",
    "print(\"(X'X)^-1 with x2:\\n{}\\n\\nWithout x2:\\n{}\\n\".format(\n",
    "    p6_xtx_1.round(10),\n",
    "    r_p6_xtx_1.round(10))\n",
    ")\n",
    "\n",
    "print(''' Final (x'x)^(-1) * x'y:\n",
    "WITH x2: {}\n",
    "WITHOUT x2: {}\n",
    "'''\n",
    ".format(p6_beta1,\n",
    "        r_p6_beta1)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Standard Error Changed: Because of the strong correlation between $x_{1}$ and $x_{2}$, the covariance matrix will contain large values on the off diagonals, vastly altering how our precision matrix $(X'X)^{-1}$ is calculated, as the strong covariances will play a part on the diagonal values. Now that we've omitted $x_{2}$ entirely we no longer worry about the large off diagonals, lowering our new $(X'X)^{-1}$ .\n",
    "\n",
    "An alternative way to see this is to look at the standard error equation for $\\beta_{1}$ $$\\text{SE}(\\boldsymbol{\\beta_{1}}) = \\frac{\\sigma}{\\sqrt{1-R^{2}_{{x-x_{1}}}\\cdot(N-1)\\cdot(\\sigma_{x_{1}})}}$$ here we see that the Standard Error is in part determined by $R^{2}_{x-x_{1}} which is the R-squared of regressing $x_{1}$ on all other x's... Considering our x's are correlated by design it makes mathematical sense our standard error were large when both covariates were included, as our $R^{2}_{x-x_{1}} was calculating by regression $x_{1}$ on $x_{2}$. Without $x_{2}$ any longer we're just regressing $x_{1}$ on some constant term (the mean)!\n",
    "\n",
    "\n",
    " For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "WITH x2: RSS avg(y-y_hat**2):0.2\n",
      "WITHOUT x2: RSS avg(y-y_hat**2): 0.4\n",
      " \n",
      "WITH x2 value of x1 diagonal in (x'x)^-1 = 6.597038722430903\n",
      "WITHOUT x2 value of x1 diagonal in (x'x)^-1 = 1.0005773324943064\n",
      "\n",
      " Taken together we calculate SE of x1 for each by doing sqrt(RSS/(n-k-1) * (x'x)^-1).\n",
      "WITH x2: sqrt(0.20\\97 * 6.597038722430903) = 0.12\n",
      "WITHOUT x2: sqrt(0.40\\98 * 1.0005773324943064) = 0.06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(''' \n",
    "WITH x2: RSS avg(y-y_hat**2):{}\n",
    "WITHOUT x2: RSS avg(y-y_hat**2): {}'''\n",
    "      .format(p6_RSS.round(2), r_p6_RSS.round(2)))\n",
    "\n",
    "print(''' \n",
    "WITH x2 value of x1 diagonal in (x'x)^-1 = {}\n",
    "WITHOUT x2 value of x1 diagonal in (x'x)^-1 = {}\n",
    "'''\n",
    "    .format(np.diag(p6_xtx_1)[1],\n",
    "            np.diag(r_p6_xtx_1)[1]))\n",
    "\n",
    "print(''' Taken together we calculate SE of x1 for each by doing sqrt(RSS/(n-k-1) * (x'x)^-1).\n",
    "WITH x2: sqrt({:.2f}\\{} * {}) = {:.2f}\n",
    "WITHOUT x2: sqrt({:.2f}\\{} * {}) = {:.2f}\n",
    "'''.format(p6_RSS, rows-2-1,np.diag(p6_xtx_1)[1], np.diag(p6_SE)[1],\n",
    "        r_p6_RSS, rows-1-1,np.diag(r_p6_xtx_1)[1], np.diag(r_p6_SE)[1]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRATCH WORK (USELESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_calc(x,y):\n",
    "    xtx_1 = np.linalg.inv(x.T@x)\n",
    "    beta1 = np.linalg.inv(x.T@x)@(x.T@y)\n",
    "    y_hat = beta1.T@x.T\n",
    "    RSS = np.sum((y-y_hat)**2)\n",
    "    RSE = np.sum(np.sqrt(RSS))\n",
    "    TSS = np.sum((y-np.mean(y))**2)\n",
    "    R2 = ((TSS-RSS)/TSS)\n",
    "    SE = np.sqrt(RSS/(x.shape[0] - x.shape[1]) * xtx_1)\n",
    "    return xtx_1, beta1, y_hat, RSS, RSE, R2, SE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create Example Data\n",
    "\n",
    "x: the x's are orthogonal to each other \\\n",
    "y: has some noise to it\n",
    "\n",
    "\n",
    "Ok so very small correlation, that shouldn't much matter but could _slightly_ alter my correlated coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanity check, x1 & x2 should have no/low correlation: 0.00023\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "rows = 100\n",
    "x = ortho_group.rvs(dim=(rows))\n",
    "x = np.append(np.full((rows,1), 1), x, axis=1)[:,:3]\n",
    "x1 = x[:, 1].reshape(rows,1)\n",
    "x2 = x[:,2].reshape(rows,1)\n",
    "y = np.dot(x, [1,1,1]) + np.random.standard_normal(rows)*0.05\n",
    "\n",
    "print('sanity check, x1 & x2 should have no/low correlation: {:.5f}'.format(np.corrcoef(x,rowvar=False)[1,2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00499049        nan 0.00153078]\n",
      " [       nan 0.04990271        nan]\n",
      " [0.00153078        nan 0.04989052]]\n",
      "my se: [0.00499049 0.04990271 0.04989052]\n",
      "my r2: 0.8998119064332305\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   435.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 17 Mar 2023</td> <th>  Prob (F-statistic):</th> <td>3.46e-49</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:49:19</td>     <th>  Log-Likelihood:    </th> <td>  159.43</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>  -312.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    97</td>      <th>  BIC:               </th> <td>  -305.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    1.0004</td> <td>    0.005</td> <td>  200.465</td> <td> 0.000</td> <td>    0.991</td> <td>    1.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    1.0570</td> <td>    0.050</td> <td>   21.180</td> <td> 0.000</td> <td>    0.958</td> <td>    1.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    1.0253</td> <td>    0.050</td> <td>   20.552</td> <td> 0.000</td> <td>    0.926</td> <td>    1.124</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.328</td> <th>  Durbin-Watson:     </th> <td>   1.785</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.312</td> <th>  Jarque-Bera (JB):  </th> <td>   1.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.188</td> <th>  Prob(JB):          </th> <td>   0.391</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.444</td> <th>  Cond. No.          </th> <td>    10.0</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.900\n",
       "Model:                            OLS   Adj. R-squared:                  0.898\n",
       "Method:                 Least Squares   F-statistic:                     435.6\n",
       "Date:                Fri, 17 Mar 2023   Prob (F-statistic):           3.46e-49\n",
       "Time:                        14:49:19   Log-Likelihood:                 159.43\n",
       "No. Observations:                 100   AIC:                            -312.9\n",
       "Df Residuals:                      97   BIC:                            -305.0\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          1.0004      0.005    200.465      0.000       0.991       1.010\n",
       "x1             1.0570      0.050     21.180      0.000       0.958       1.156\n",
       "x2             1.0253      0.050     20.552      0.000       0.926       1.124\n",
       "==============================================================================\n",
       "Omnibus:                        2.328   Durbin-Watson:                   1.785\n",
       "Prob(Omnibus):                  0.312   Jarque-Bera (JB):                1.876\n",
       "Skew:                           0.188   Prob(JB):                        0.391\n",
       "Kurtosis:                       2.444   Cond. No.                         10.0\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtx_1, beta1, y_hat, RSS, RSE, R2, SE = hand_calc(x,y)\n",
    "print(SE)\n",
    "\n",
    "model = sm.OLS(y,x).fit()\n",
    "\n",
    "print('my se:', np.diag(SE))\n",
    "print('my r2:', R2)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIF is 1/(how well we can explain on covariates with the others).\n",
    "\n",
    "Or \n",
    "\n",
    "$\\frac{1}{R^{2}_{X_{j}|X_{-j}}}$\n",
    "\n",
    "Where we calculate the R-squared of a regression of $X_{j}$ on all other covariates $X_{-j}$.\n",
    "\n",
    "A value of 1 means no collinear relationships, where a value greater than 15ish is a cause for concern. \n",
    "\n",
    "Obviously I made these covariates to not vary with each other so the values are 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF Scores:  [1.0006659700128582, 1.0000000511142828, 1.0000000511142828]\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "print('VIF Scores: ',[variance_inflation_factor(x, i) for i in range(3)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What occurs when I take out an orthogonal, yet significant explanatory variable?\n",
    "\n",
    "What we know:\n",
    "1. x2 DOES explain our Y.\n",
    "2. x1 & x2 are effectively not correlated in any way.\n",
    "\n",
    "\n",
    "What should occur to x1 & why:\n",
    "1. x1 beta coefficient should stay the same, the precision matrix $(x'x)^{-1}$ should be unaffected as $x1$ and $x2$ had little to no relation, therefore, when we take $x2$ out the variance/covariance of this matrix should stay very much the same.\n",
    "2. x1 se should increase as our RSE has increased. x2 provided strong explanatory power we're now missing.\n",
    "2. x1 se should decrease as our number of variables have decreased so n-k-1, where k went from 2 to 1, meaning our denominator is smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my coef: [1.00042103 1.05696313 1.02533166]\n",
      "my se: [0.00499049 0.04990271 0.04989052]\n",
      "my r2: 0.8998119064332305\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.458</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   84.69</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 17 Mar 2023</td> <th>  Prob (F-statistic):</th> <td>6.51e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:49:19</td>     <th>  Log-Likelihood:    </th> <td>  75.531</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>  -147.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>  -141.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.9995</td> <td>    0.011</td> <td>   86.999</td> <td> 0.000</td> <td>    0.977</td> <td>    1.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    1.0572</td> <td>    0.115</td> <td>    9.202</td> <td> 0.000</td> <td>    0.829</td> <td>    1.285</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 4.361</td> <th>  Durbin-Watson:     </th> <td>   1.986</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.113</td> <th>  Jarque-Bera (JB):  </th> <td>   5.475</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.078</td> <th>  Prob(JB):          </th> <td>  0.0647</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.136</td> <th>  Cond. No.          </th> <td>    10.0</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.464\n",
       "Model:                            OLS   Adj. R-squared:                  0.458\n",
       "Method:                 Least Squares   F-statistic:                     84.69\n",
       "Date:                Fri, 17 Mar 2023   Prob (F-statistic):           6.51e-15\n",
       "Time:                        14:49:19   Log-Likelihood:                 75.531\n",
       "No. Observations:                 100   AIC:                            -147.1\n",
       "Df Residuals:                      98   BIC:                            -141.9\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.9995      0.011     86.999      0.000       0.977       1.022\n",
       "x1             1.0572      0.115      9.202      0.000       0.829       1.285\n",
       "==============================================================================\n",
       "Omnibus:                        4.361   Durbin-Watson:                   1.986\n",
       "Prob(Omnibus):                  0.113   Jarque-Bera (JB):                5.475\n",
       "Skew:                           0.078   Prob(JB):                       0.0647\n",
       "Kurtosis:                       4.136   Cond. No.                         10.0\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_x = x[:,0:2]\n",
    "r_xtx_1, r_beta1, r_y_hat, r_RSS, r_RSE, r_R2, r_SE = hand_calc(r_x,y)\n",
    "model = sm.OLS(y,r_x).fit()\n",
    "print('my coef:', beta1)\n",
    "print('my se:', np.diag(SE))\n",
    "print('my r2:', R2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After omitting x2, my x1 went from 1.0570 to 1.0572, a difference of -0.0002.\n",
      "\n",
      "\n",
      "The reason my x1 change after omitting x2 is due to the fact that I calculate my beta by (X'X)^(-1)X'Y, \n",
      "because x1 and x2 share a slight covariance with each other (-.0002), this changes my (X'X)^(-1) matrix slightly, as such:\n",
      "\n",
      "original (X'X)^-1:\n",
      "[[ 0.01000666 -0.00240368  0.00094152]\n",
      " [-0.00240368  1.00057738 -0.00022616]\n",
      " [ 0.00094152 -0.00022616  1.00008859]]\n",
      "\n",
      "reduced form:\n",
      "[[ 0.01000577 -0.00240347]\n",
      " [-0.00240347  1.00057733]]\n",
      "\n",
      "As you can see, the x'y remains unchanged:\n",
      "(x'y):\n",
      "[100.1995   1.2973   0.9312]\n",
      "\n",
      "reduced form (x'y):\n",
      "[100.1995   1.2973]\n"
     ]
    }
   ],
   "source": [
    "print('''After omitting x2, my x1 went from {:.4f} to {:.4f}, a difference of {:.4f}.\n",
    "'''.format(beta1[1], r_beta1[1], beta1[1]-r_beta1[1]))\n",
    "                                                                          \n",
    "\n",
    "print('''\n",
    "The reason my x1 change after omitting x2 is due to the fact that I calculate my beta by (X'X)^(-1)X'Y, \n",
    "because x1 and x2 share a slight covariance with each other (-.0002), this changes my (X'X)^(-1) matrix slightly, as such:\n",
    "''')\n",
    "\n",
    "print(\"original (X'X)^-1:\\n{}\\n\\nreduced form:\\n{}\\n\".format(\n",
    "    xtx_1.round(10),\n",
    "    r_xtx_1.round(10))\n",
    ")\n",
    "\n",
    "print(\"As you can see, the x'y remains unchanged:\")\n",
    "print(\"(x'y):\\n{}\\n\\nreduced form (x'y):\\n{}\".format(\n",
    "    (x.T@y).round(4),\n",
    "    (r_x.T@y).round(4))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The standard error of x1 went from 0.05 to 0.115.\n",
      "\n",
      "The standard error changed, why?\n",
      "Well standard error is computed by sqrt(RSS/(n-k-1) * (x'x)^-1).\n",
      "Where RSS = variance of our error, the squared difference of the true y and our estimated y.\n",
      "So sqrt(RSS\n",
      "-k-1) is the standard error of our variance.\n",
      "\n",
      "Lets just call sqrt(RSS\n",
      "-k-1) as sigma...\n",
      "\n",
      "\n",
      "sigma has changed for a few reasons:\n",
      "1. SE grows: RSS = (y-y_hat)^2... Because we our y_hat is noisier without x2 we will expect RSS to become larger as the distance between any given y and y_hat has grown\n",
      "2. SE grows:  k was 2 (x1, x2), now it is 1 (x1 only). This will reduce sigma as our numerator has become larger ever so slightly \n",
      "\n",
      "\n",
      "We then apply that to (x'x)^-1:\n",
      "3. SE grows: because x1 and x2 had an slight correlation we will see an ever so slight change in our values here.\n",
      " \n",
      "-- POINT 1 --\n",
      "WITH x2: RSS avg(y-y_hat**2):0.24\n",
      "WITHOUT x2: RSS avg(y-y_hat**2): 1.29\n",
      " \n",
      "-- POINT 2 --\n",
      "WITH x2 RSS\\(n-2-1) = 0.24\\97 = 0.002\n",
      "WITHOUT x2 RSS\\(n-1-1) = 1.29\\98 = 0.013\n",
      "\n",
      " \n",
      "-- POINT 3 --\n",
      "WITH x2 value of x1 diagonal in (x'x)^-1 = 1.0005773836380991\n",
      "WITHOUT x2 value of x1 diagonal in (x'x)^-1 = 1.0005773324943064\n",
      "\n",
      " Taken together we calculate SE of x1 for each by doing sqrt(RSS/(n-k-1) * (x'x)^-1).\n",
      "WITH x2: sqrt(0.24\\97 * 1.0005773836380991) = 0.05\n",
      "WITHOUT x2: sqrt(1.29\\98 * 1.0005773324943064) = 0.11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "The standard error of x1 went from {np.diag(SE)[1].round(3)} to {np.diag(r_SE)[1].round(3)}.\n",
    "\n",
    "The standard error changed, why?\n",
    "Well standard error is computed by sqrt(RSS/(n-k-1) * (x'x)^-1).\n",
    "Where RSS = variance of our error, the squared difference of the true y and our estimated y.\n",
    "So sqrt(RSS\\n-k-1) is the standard error of our variance.\n",
    "\n",
    "Lets just call sqrt(RSS\\n-k-1) as sigma...\n",
    "\n",
    "\n",
    "sigma has changed for a few reasons:\n",
    "1. SE grows: RSS = (y-y_hat)^2... Because we our y_hat is noisier without x2 we will expect RSS to become larger as the distance between any given y and y_hat has grown\n",
    "2. SE grows:  k was 2 (x1, x2), now it is 1 (x1 only). This will reduce sigma as our numerator has become larger ever so slightly \n",
    "\n",
    "\n",
    "We then apply that to (x'x)^-1:\n",
    "3. SE grows: because x1 and x2 had an slight correlation we will see an ever so slight change in our values here.''')\n",
    "\n",
    "\n",
    "print(''' \n",
    "-- POINT 1 --\n",
    "WITH x2: RSS avg(y-y_hat**2):{}\n",
    "WITHOUT x2: RSS avg(y-y_hat**2): {}'''\n",
    "      .format(RSS.round(2), r_RSS.round(2)))\n",
    "\n",
    "print(''' \n",
    "-- POINT 2 --\n",
    "WITH x2 RSS\\(n-2-1) = {:.2f}\\{} = {:.3f}\n",
    "WITHOUT x2 RSS\\(n-1-1) = {:.2f}\\{} = {:.3f}\n",
    "'''\n",
    "    .format(RSS, rows-2-1, RSS/(rows-2-1),\n",
    "            r_RSS, rows-1-1, r_RSS/(rows-1-1)))\n",
    "\n",
    "print(''' \n",
    "-- POINT 3 --\n",
    "WITH x2 value of x1 diagonal in (x'x)^-1 = {}\n",
    "WITHOUT x2 value of x1 diagonal in (x'x)^-1 = {}\n",
    "'''\n",
    "    .format(np.diag(xtx_1)[1],\n",
    "            np.diag(r_xtx_1)[1]))\n",
    "\n",
    "print(''' Taken together we calculate SE of x1 for each by doing sqrt(RSS/(n-k-1) * (x'x)^-1).\n",
    "WITH x2: sqrt({:.2f}\\{} * {}) = {:.2f}\n",
    "WITHOUT x2: sqrt({:.2f}\\{} * {}) = {:.2f}\n",
    "'''.format(RSS, rows-2-1, np.diag(xtx_1)[1], np.diag(SE)[1],\n",
    "        r_RSS, rows-1-1, np.diag(r_xtx_1)[1], np.diag(r_SE)[1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$SE(\\hat{\\beta_j}) = \\sqrt{\\frac{\\hat{\\sigma}^2}{\\sum_{i=1}^{n}(x_{ij} - \\bar{x_j})^2(1-R_j^2)}}$$\n",
    "\n",
    "$$\\sqrt{\\hat{\\sigma}^{2}(X'X)^{-1}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_minus_xbar = np.var(x_6[:,1])*rows\n",
    "r2 = sm.OLS(x_6[:,1],x_6[:,[0,2]]).fit().rsquared\n",
    "\n",
    "\n",
    "xtx_1, beta1, y_hat, RSS, RSE, R2, SE = hand_calc(x_6[:,[0,2]], x_6[:,1])\n",
    "\n",
    "xtx = x_6[:,[0,2]].T@x_6[:,[0,2]]\n",
    "\n",
    "TSS = np.sum((x_6[:,1]-np.mean(x_6[:,1]))**2)\n",
    "RSS = np.sum((x_6[:,1]-y_hat)**2)\n",
    "R2 = ((TSS-RSS)/TSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11430720562798564"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se_b1 = np.sqrt(p6_RSS/((TSS)*(1-((TSS-RSS)/TSS))*(rows-1)))\n",
    "se_b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0063914,       nan],\n",
       "       [      nan, 0.063914 ]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_p6_SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variance of x is 1890.54\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the parameters for the regression equation\n",
    "beta_0 = 5    # intercept\n",
    "beta_1 = 2    # slope\n",
    "n = 100       # sample size\n",
    "\n",
    "# Generate the explanatory variable x with varying variance\n",
    "variance = 2000\n",
    "x = np.random.normal(loc=0, scale=np.sqrt(variance), size=n)\n",
    "\n",
    "# Calculate the variance of x\n",
    "x_var = np.var(x)\n",
    "\n",
    "# Generate the response variable y using the regression equation\n",
    "y = beta_0 + beta_1 * x + np.random.normal(loc=0, scale=1, size=n)\n",
    "\n",
    "# Fit the regression model using numpy's polyfit function\n",
    "slope, intercept = np.polyfit(x, y, 1)\n",
    "\n",
    "# Plot the data and the fitted regression line\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, alpha=0.8)\n",
    "ax.plot(x, slope * x + intercept, color='red')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Simple Linear Regression')\n",
    "\n",
    "# Print the variance of x\n",
    "print(f\"The variance of x is {x_var:.2f}\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7ded92a62eb8d00bc6880547b1023ec86ee52a34e12c88218e87fc5e1f5ea50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
