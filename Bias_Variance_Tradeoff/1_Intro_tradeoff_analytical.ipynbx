{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A primer on The Bias - Variance Trade-Off\n",
    "\n",
    "This is inspired by [Cornell's machine learning course](https://www.cs.cornell.edu/courses/cs4780/2022sp/). This summarizes the bias variance trade off, ignoring the heavier parts of probability theory in the process. This is aimed to help students adept with math and statistics better understand the trade off.\n",
    "\n",
    "TODO (4/23/2022):\n",
    "* Data example\n",
    "* Check Grammar\n",
    "* Make section 4 better (I got tired lol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting the Stage\n",
    "\n",
    "Before we begin we need to define some notation & set the context:\n",
    "\n",
    "* Everything below is with _regression_ in mind. While the general idea is true for classification we would need to change some terms and ideas around.\n",
    "\n",
    "* We consider a training dataset $D = \\{(x_1, y_1), ..., (x_n,y_n)\\}$ where $x$ can be a vector of predictors and $y$ is our outcome.\n",
    "\n",
    "* $D$ is drawn i.i.d. (each observation is independent of the other, no worries on correlation across obs.) from a distribution $P(X,Y)$.\n",
    "\n",
    "* $P$ can be thought of as the space where we can draw possible data points. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Label\n",
    " Even if two sets of predictors are identical ($x_k = x_n$), it does not imply that their outcomes are identical ($y_k \\not= y_n$). For instance, two homes can have identical features - same square footage, rooms, and location - but sell for different prices. Therefore, we can define an expected label given a set of predictors:\n",
    "\n",
    "$$\n",
    "\\bar{y}(x) = E_{y|x}[Y]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining $h$ & $\\mathcal{A}$ \n",
    "\n",
    "With our data $D$ and our expected label definition in mind, we need to some machine learning algorithm to build a model that can train on some $D$ and take subsequent unseen data and predict an outcome for us, this is the whole point of machine learning. Therefore:\n",
    "\n",
    "* $\\mathcal{A}$ is our machine learning algorithm, that could be decision trees, linear regressions, logistic regressions, neural nets, letting a octopus pick a outcome for you, etc...\n",
    "\n",
    "Given $\\mathcal{A}$, we can feed it data $D$ to create our model. In this case the model we build, i.e. the thing that can take in new data and spit our predictions is called our hypothesis/classifier/model, we call it:\n",
    "\n",
    "* $h_D$ is the hypothesis. This is the model built after feeding our algorithim $\\mathcal{A}$ data. Formally: $h_D = \\mathcal{A}(D)$.\n",
    "\n",
    "\n",
    "$h_D$ can be given new test data $x$ and create a prediction $h_D(x) = \\hat{y}$. This means we can define the model's efficacy with a loss function...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Expected Test Error (given $h_D$):\n",
    "\n",
    "So as machine learners working on a project we are given data $D$, we explore the data via EDA, and we decide to try out an algorithm $\\mathcal{A}$. To do this we feed data into the algorithm, i.e. $\\mathcal{A}(D)$ and learn a hypothesis (make a model) $h_D$. $h_D$ is capable of taking in new test data and outputting a prediction $h_D(x) = \\hat{y}$, but to understand how well this prediction is we need to find an expected test error:\n",
    "\n",
    "$$\n",
    "E_{(x,y)\\sim P}[(h_D(x)-y)^2]\n",
    "$$\n",
    "\n",
    "This error is the squared lost (MSE) and we can break it down to simpler terms:\n",
    "\n",
    "* $E_{(x,y)\\sim P}$: This says we want the expected value of the actual points we have and we're testing on $(x,y)$ taken from a distribution we got them from called $P$. \n",
    "* $h_D(x)$: This is the equivalent of taking our machine learning model $h_D$ we made by giving it training data $D$. We feed this model test data $x$. Therefore, $h_D(x)$ is the same as $\\hat{y}$.\n",
    "* $(h_D(x)-y)^2$: This is asking for the difference between the outcome our model predicts for the testing data and what the actual outcome is i.e.  $h_D(x) = \\hat{y}$, so.... $(h_D(x)-y)^2 == (\\hat{y}-y)^2$.\n",
    "\n",
    "\n",
    "An important fact to remember is that $h_D$ was made with data training data $D$. $D$ is taken from a distribution $P$. If we took $D$ from $P$ again then the new dataset might not be identical to the old dataset, meaning the model we make might be different. This leads us into the last point:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Expected Classifier (given $\\mathcal{A}$):\n",
    "\n",
    "Before I jump in lets take a small example to motivate this point:\n",
    "\n",
    "Imagine you want to predict home prices in San Diego, then...\n",
    "\n",
    "* $D$ data composed of a set of predictors $x$ such as bedrooms, and home price $y$.\n",
    "* $P$ are the distributions of predictors and prices across San Diego County.\n",
    "\n",
    "Let's say you collect data two times, so you have datasets $D_1$ and $D_2$. These two datasets are similar but not identical, maybe one dataset has more data points in a richer area of San Diego and the other has more from a poorer area.\n",
    "\n",
    "Next you train a model using a decision tree, so $\\mathcal{A}$ is a decision tree. \n",
    "\n",
    "You get two different models using the different datasets: $h_{D_1}$ and $h_{D_2}$. \n",
    "\n",
    "Further, the expected test error are different as each model was trained on different data.\n",
    "\n",
    "Therefore, due to the random nature of the distribution of data and data collection it's very possible identical algorithms and produce different test errors. This leads to the expected classifier. The idea here is that given countless draws from $P$ for many datasets $D$ the average model we build will have some expected error:\n",
    "\n",
    "$$\n",
    "\\bar{h} = E_{D\\sim P^n}[h_D]\n",
    "$$\n",
    "\n",
    "This is essentially saying that our average model is $\\bar{h}$ which is the same as the expected value of $h_D$ given that $D$ is arbitrarily taken from the distribution $P^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Expected Test Error (given $\\mathcal{A}$):\n",
    "\n",
    "Didn't we already define the expected test error? Well yes, we did for a specific model $h_D$ given specific data $D$, but we want to generalize this across any and all possible models we could build given the random nature of data we could pull to build it. Therefore below defines the expected error of our algorithm $A$:\n",
    "\n",
    "$$\n",
    "\\underset{D \\sim P^n}{E_{(x,y)\\sim P}}[(h_D(x)-y)^2]\n",
    "$$\n",
    "\n",
    "This looks very similar to the previous definition except we now have $D \\sim P^n$ under the expected value, meaning for any dataset drawn from our distribution. So to break it down step by step:\n",
    "\n",
    "1. Draw a training dataset $D$ from distribution $P$\n",
    "2. Train our algorithm $\\mathcal{A}$ to create our model $h_D$.\n",
    "3. Take a test set of data $(x,y)$ from distribution $P$\n",
    "4. Measure expected error of $[(h_D(x)-y)^2]$.\n",
    "\n",
    "If we were to do this millions of times, via the law of large numbers, we would get the expected test error of our algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decomposition of Expected Test Error\n",
    "\n",
    "Now we break our expected test error of our algorithm to understand bias and variance. Let's go step by step:\n",
    "\n",
    "* Start with $\\underset{D \\sim P^n}{E_{(x,y)\\sim P}}[(h_D(x)-y)^2]$\n",
    "\n",
    "* Add and subtract the average classifier/model given test point $x$, $\\bar{h}(x)$:\n",
    "\n",
    "$$\n",
    "\\underset{D \\sim P^n}{E_{(x,y)\\sim P}}[(h_D(x)- \\bar{h}(x) + \\bar{h}(x) -y)^2]\n",
    "$$\n",
    "\n",
    "* lets just make stuff simpler, lets put some parenthesis around terms and rewrite $E$ to $E_{x,y,D}$, meaning expected value with respects to the specific $x, y,$ and $D$ we're dealing with.\n",
    "\n",
    "$$\n",
    "E_{x,y,D}[[(h_D(x)-\\bar{h}(x)) + (\\bar{h}(x)-y)]^2]\n",
    "$$\n",
    "\n",
    "* Lets distribute everything as the whole term is squared:\n",
    "\n",
    "$$\n",
    "E_{x,D}[(h_D(x)-\\bar{h}(x))^2] + E_{x,y}[(\\bar{h}(x)-y)^2] + 2E_{x,y,D}[(h_D-\\bar{h}(x))(\\bar{h}(x)-y)]\n",
    "$$\n",
    "\n",
    "* Lets look at the last term $2E[(h_D-\\bar{h}(x))(\\bar{h}(x)-y)]$, what does it equal?\n",
    "\n",
    "* Well $E_D[h_D] = \\bar{h}$, so this means we can rewrite this to:\n",
    "\n",
    "$$\n",
    "2E_{x,y}[\\bar{h}(x)-\\bar{h}(x)]\n",
    "$$\n",
    "\n",
    "So we actually have \n",
    "$$\n",
    "2[(0)(\\bar{h}(x)-y))] == 0\n",
    "$$\n",
    "\n",
    "\n",
    "* After cancelling the last term we are left with:\n",
    "\n",
    "$$\n",
    "E_{x,D}[(h_D(x)-\\bar{h}(x))^2] + E_{x,y}[(\\bar{h}(x)-y)^2] \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking Down the Two Terms in the Decomposition $E_{x,D}[(h_D(x)-\\bar{h}(x))^2]$ & $E_{x,y}[(\\bar{h}(x)-y)^2]$ :\n",
    "\n",
    "What do we make of the two last terms after the decomposition? Let's go further:\n",
    "\n",
    "First, looking at $E_{x,D}[(h_D(x)-\\bar{h}(x))^2]$ it looks like variance ($V = (X-\\bar{X})^2$), so we can interpret this as the variance of our model/classifier we built $h_D$.\n",
    "\n",
    "Second, looking at $E_{x,y}[(\\bar{h}(x)-y)^2]$ it seems like its the measures the average prediction for our test point $\\bar{h}(x)$ by its actual value $y$ squared, however lets break this down further with another add/subtract trick:\n",
    "\n",
    "* First add and subtract the average outcome given a set of predictors $x$, $\\bar{y}(x)$:\n",
    "$$\n",
    "E[[(\\bar{h}(x)-\\bar{y}(x)) + (\\bar{y}(x)-y)]^2]\n",
    "$$\n",
    "\n",
    "* Just like before we expand this term as the whole term is squared:\n",
    "\n",
    "$$\n",
    "E[(\\bar{h}(x)-\\bar{y}(x))^2] + E[(\\bar{y}(x)-y))^2] + 2(\\bar{h}(x)-\\bar{y}(x))(\\bar{y}(x)-y))\n",
    "$$\n",
    "\n",
    "* Just like before, the last term is equal to 0 (if you want to see this in-depth you can go [here](https://www.cs.cornell.edu/courses/cs4780/2022sp/notes/LectureNotes17.html). So we can get rid of the last term to now have:\n",
    "\n",
    "$$\n",
    "E_x[(\\bar{h}(x)-\\bar{y}(x))^2] + E_{x,y}[(\\bar{y}(x)-y))^2]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identifying Each Error Term\n",
    "\n",
    "What's left from the expected test error? We have three terms:\n",
    "\n",
    "1. $E_{x,D}[(h_D(x)-\\bar{h}(x))^2]$, which was the first term from the initial decomposition. It looks like the definition of variance.\n",
    "2. $E_{x,y}[(\\bar{h}(x)-\\bar{y}(x))^2]$ which is the first term from the second decomposition. It looks like the difference between how the average classifier/model determines what test point $x$'s outcome is and the average outcome given $x$.\n",
    "3. $E_{x}[(\\bar{y}(x)-y))^2]$ which is the last term from the second decomposition. It looks like noise, the average outcome given $x$ can deviate from a specific observation of $x$ (like when building a linear relationship between # of rooms & home price, even if we have all the data in the world, there will still be some distance between the average price for a 2 room home and a specific observation of a 2 room home).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Therefore the expected error is:\n",
    "\n",
    "$$\n",
    "\\underset{D \\sim P^n}{E_{(x,y)\\sim P}}[(h_D(x)-y)^2] = E_{x,D}[(h_D(x)-\\bar{h}(x))^2] + E_{x,y}[(\\bar{h}(x)-\\bar{y}(x))^2] + E_{x}[(\\bar{y}(x)-y))^2]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text is taken from Cornell website (I really like its clarity)\n",
    "\n",
    "### Variance: $E_{x,D}[(h_D(x)-\\bar{h}(x))^2]$\n",
    "\n",
    "Captures how much your classifier changes if you train on a different training set. How \"over-specialized\" is your classifier to a particular training set (overfitting)? If we have the best possible model for our training data, how far off are we from the average classifier?\n",
    "\n",
    "\n",
    "### Noise (Irreducible Error): $E_{x,y}[(\\bar{y}(x)-y))^2]$ \n",
    "\n",
    "For a certain $x$ we have an outcome $y$, further we can find the expected value $\\bar{y}(x)$ by taking the average of all instances of $x$ that share a similar $y$. Deviations between a specific instance of $x$ and the average $\\bar{y}(x)$ comes from our data. A larger deviation between the two means our data is noiser, as what we expect and what we observe are far apart. As we gather more and more data we can update $\\bar{y}(x)$ to better match each specific $x$ we observe, reducing the noise. Regardless, this error is not due to our classifier/model, but to the data itself.\n",
    "\n",
    "\n",
    "### Bias-squared: $E_{x}[(\\bar{h}(x)-\\bar{y}(x))^2]$\n",
    "\n",
    "What is the inherent error that you obtain from your classifier even with infinite training data? This is due to your classifier being \"biased\" to a particular kind of solution (e.g. linear classifier). In other words, bias is inherent to your model.\n",
    "\n",
    "In more detail:\n",
    "* Remember that you pick an algorithm $\\mathcal{A}$. $\\mathcal{A}$ has two components that matter here - the algorithm itself (OLS, Logistic, Tress, etc), and the hyperparameters (variables chosen, depth, exponential terms, etc...). \n",
    "* The chosen algorithm will have an expected classifier/model we call $\\bar{h}(x)$, that no matter how much data you have, it will always take test point $x$ and output $\\bar{h}(x)$\n",
    "* Imagine the true relationship between $x$ and $y$ follows some relationship $f$ and that the algorithm or hyperparameter you chose is just poorly equipped to explain such a relationship (like $x$ is age and $y$ is ability to lift heavy objects and you picked linear regression with no polynomial terms)\n",
    "* In this case no matter what you do in terms of data available, train test split, etc... the error is driven by the poor choice in model choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. A Qualitative Approach to the Bias - Variance Trade Off:\n",
    "\n",
    "To end (as of 4/23/2022, I can't write all of this in a day!) this discussion on the expected test error I wanted to give a qualitative explanation of the bias variance tradeoff. Countless articles online can show you better explanations.\n",
    "\n",
    "\n",
    "Why is there a bias-variance trade off in the first place? Lets look at some stylized facts:\n",
    "\n",
    "### General Facts\n",
    "* More data will HELP LOWER VARIANCE! ! !\n",
    "* We have the power to dictate what $x$ is composed of. For instance, if $x$ are features of a home, we have the power to include features on square footage, number of rooms, occupant size, etc...\n",
    "* Sub-scripts matter! Play attention to them!\n",
    "\n",
    "### Variance \n",
    "* For variance we have $E_{x,D}$, this means we're looking at a specific _test vector_ $x$ and a specific _training data-set_ $D$.  \n",
    "* We can read $h_D(x)$ as a classifier/model we built using only _training data_ $D$ and we're inputting a _unseen, new vector of values_ $x$.\n",
    "* Remember, $\\bar{h}(x)$ has __NO__ subscript $D$, meaning that $\\bar{h}(x)$ is referring to the average classifier/model across a world where we build a model with _all data_, not just the training data.\n",
    "* Therefore $E_{x,D}[(h_D(x)-\\bar{h}(x))^2]$ is comparing our \"specialized to the training data\" model $h_D(x)$ against the average model $\\bar{h}(x)$.\n",
    "* If our data $D$ is small in size, very unique as compared to other data in the distribution $P$, or the algorithm we specify be highly complex to our data we might see very __HIGH__ variance.\n",
    "* If we have a lot of data to use, then we could expect the variance to lower.\n",
    "\n",
    "### Bias\n",
    "* For bias we have $E_{x}$, this means bias is __not affected__ by what our _training data-set_ $D$ looks like, but only what _test vector_ $x$ we're looking at.\n",
    "* Remember, $\\bar{h}(x)$ is crafted by our choice of algorithm and hyperparameter. Changing the algorithm, hyperparameter, or composition of $x$ are the _only ways_ to alter $\\bar{h}(x)$. DATA DOES NOT MATTER HERE! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Variance, Low Bias:\n",
    "* Any algorithm $\\mathcal{A}$ goal is to minimize loss, so $h_D$ is made to minimize loss on the data points in the training data $D$.\n",
    "* High variance means the distance between $h_D(x)$ and $\\bar{h}(x)$ is large. This implies that when given the same test point $x$ our model built on training data $h_D$ is very very different than the average model $\\bar{h}$.\n",
    "* Low bias means the distance between the average model $\\bar{h}(x)$ and average outcome given $x$, $\\bar{y}(x)$ is small. This implies the algorithm and hyperparameter we used is extremely adept at explaining some latent relationship between $x$ and $y$.\n",
    "* __In Sum__: Having high variance means the model we made is highly specialized to minimize loss over the data we give it, however the data we gave it isn't extremely similar to other data in the world. However, the bias error doesn't rely on what data is used, so using this high variance model $h_D$ and looking at the _average in the world where we had infinite data_ ($D$ is infinite!) means bias will be low.\n",
    "\n",
    "### Low Variance, High Bias:\n",
    "* Low variance means the distance between $h_D(x)$ and $\\bar{h}(x)$ is small. This implies that when given the same test point $x$ our model built on training data $h_D$ is quite similar than the average model $\\bar{h}$.\n",
    "* High bias means the distance between the average model $\\bar{h}(x)$ and average outcome given $x$, $\\bar{y}(x)$ is large. This implies that even with all the data in the world the algorithm we specified is missing the mark on explains the relationship between $x$ and $y$\n",
    "* __In Sum__: Having high bias means the model fails to uncover the latent relationship between $x$ and $y$, creating a gap between the average outcome and the outcome the average model crafts. But as the model fails to predict well, it also means it fails to overspecialized on the training data making the distance between $h_D$ and $\\bar{h}(x)$ small.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3472633a0c3319afea4dee893cd914becdcc9b59b8cbd89385fb4e7aafbcbb11"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
